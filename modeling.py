# modeling_updated.py
import pandas as pd
import numpy as np
from datetime import timedelta
import joblib
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ tqdm Ð´Ð»Ñ pandas
tqdm.pandas()

print("ðŸš€ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
df_fraud = pd.read_parquet('data/transaction_fraud_data.parquet')
df_exchange = pd.read_parquet('data/historical_currency_exchange.parquet')

# --- 1. Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ Ð¿Ñ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ---
print("ðŸ”§ ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ timestamp Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²...")
df_fraud['timestamp'] = pd.to_datetime(df_fraud['timestamp'])
df_fraud['hour'] = df_fraud['timestamp'].dt.hour
df_fraud['is_weekend'] = df_fraud['timestamp'].dt.dayofweek >= 5  # Ð¡Ð±=5, Ð’Ñ=6
df_fraud['date'] = df_fraud['timestamp'].dt.date

# Ð Ð°ÑÐ¿Ð°ÐºÐ¾Ð²ÐºÐ° last_hour_activity
df_activity = pd.json_normalize(df_fraud['last_hour_activity'])
df_activity.columns = [f"last_hour_{col}" for col in df_activity.columns]
df_fraud = pd.concat([df_fraud.drop(columns=['last_hour_activity']), df_activity], axis=1)

# --- 2. ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ Ð² USD ---
print("ðŸ’± ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ ÑÑƒÐ¼Ð¼Ñ‹ Ð² USD...")
df_exchange['date'] = pd.to_datetime(df_exchange['date']).dt.date
currencies = [col for col in df_exchange.columns if col != 'date']
exchange_stacked = df_exchange.melt(id_vars='date', value_vars=currencies,
                                    var_name='currency', value_name='exchange_rate_to_usd')
usd_row = pd.DataFrame({'date': df_exchange['date'].unique(),
                        'currency': 'USD', 'exchange_rate_to_usd': 1.0})
exchange_stacked = pd.concat([exchange_stacked, usd_row], ignore_index=True)

df_fraud = df_fraud.merge(exchange_stacked, on=['date', 'currency'], how='left')
df_fraud['amount_usd'] = df_fraud['amount'] * df_fraud['exchange_rate_to_usd']

print(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹. Ð Ð°Ð·Ð¼ÐµÑ€: {df_fraud.shape}")

# --- 3. Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð´Ð»Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² ---
df_fraud = df_fraud.sort_values(['customer_id', 'timestamp']).reset_index(drop=True)

# --- 4. Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ EDA ---

print("âš™ï¸ Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²...")

# 4.1. Ð’Ñ€ÐµÐ¼Ñ Ñ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¹ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ (Ð² Ñ‡Ð°ÑÐ°Ñ…)
df_fraud['time_since_last_transaction'] = (
    df_fraud.groupby('customer_id')['timestamp'].diff().dt.total_seconds() / 3600
)
df_fraud['time_since_last_transaction'].fillna(0, inplace=True)

# 4.2. Ð’Ð¾Ð·Ñ€Ð°ÑÑ‚ Ð°ÐºÐºÐ°ÑƒÐ½Ñ‚Ð° (Ð² Ð´Ð½ÑÑ…) â€” Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸Ñ, Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¸Ð· Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ñ
np.random.seed(42)
df_fraud['account_age_days'] = np.random.lognormal(mean=4, sigma=1.5, size=len(df_fraud)).astype(int)
df_fraud['account_age_days'] = df_fraud['account_age_days'].clip(1, 3650)

# 4.3. Ð”Ð¾Ð»Ð³Ð¾Ðµ Ð±ÐµÐ·Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ (>30 Ð´Ð½ÐµÐ¹)
df_fraud['prev_timestamp'] = df_fraud.groupby('customer_id')['timestamp'].shift(1)
df_fraud['inactive_days'] = (df_fraud['timestamp'] - df_fraud['prev_timestamp']).dt.total_seconds() / (3600 * 24)
df_fraud['inactive_days'] = df_fraud['inactive_days'].clip(0, 365)

# 4.4. ÐœÐ½Ð¾Ð³Ð¾ ÑÑ‚Ñ€Ð°Ð½ Ð·Ð° Ñ‡Ð°Ñ â†’ Ð¿Ð¾Ð´Ð¾Ð·Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾
df_fraud['is_multi_country_hour'] = (df_fraud['last_hour_unique_countries'] >= 2).astype(int)

# 4.5. Ð’Ñ‹ÑÐ¾ÐºÐ¸Ð¹ Ð²ÑÐ¿Ð»ÐµÑÐº Ð¼ÐµÑ€Ñ‡Ð°Ð½Ñ‚Ð¾Ð² â†’ card testing
df_fraud['is_high_merchant_burst'] = (df_fraud['last_hour_unique_merchants'] >= 5).astype(int)

# 4.6. Ð’Ñ‹ÑÐ¾ÐºÐ¾Ñ€Ð¸ÑÐºÐ¾Ð²Ð°Ñ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ñ: Ð¾Ð½Ð»Ð°Ð¹Ð½ + CNP + Ð·Ð° Ð³Ñ€Ð°Ð½Ð¸Ñ†ÐµÐ¹
df_fraud['is_high_risk_combo'] = (
    (df_fraud['vendor_type'] == 'Ð¾Ð½Ð»Ð°Ð¹Ð½') &
    (~df_fraud['is_card_present']) &
    (df_fraud['is_outside_home_country'])
).astype(int)

# 4.7. ÐžÑ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð¾Ñ‚Ð¿ÐµÑ‡Ð°Ñ‚ÐºÐ° ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°
df_fraud['has_device_fingerprint'] = df_fraud['device_fingerprint'].notna().astype(int)

# 4.8. Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ð¸ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð¾Ðµ Ð¾Ñ‚ÐºÐ»Ð¾Ð½ÐµÐ½Ð¸Ðµ Ñ€Ð°ÑÑ…Ð¾Ð´Ð¾Ð² (Ñ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€Ð¾Ð¼)
print("ðŸ“Š Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ past_avg_amount Ð¸ past_std_amount...")
df_fraud['past_avg_amount'] = df_fraud.groupby('customer_id')['amount_usd'].progress_apply(
    lambda x: x.shift(1).expanding().mean()
).reset_index(level=0, drop=True)
df_fraud['past_std_amount'] = df_fraud.groupby('customer_id')['amount_usd'].progress_apply(
    lambda x: x.shift(1).expanding().std()
).reset_index(level=0, drop=True)
df_fraud['past_avg_amount'].fillna(df_fraud['amount_usd'].median(), inplace=True)
df_fraud['past_std_amount'].fillna(df_fraud['amount_usd'].std(), inplace=True)

df_fraud['spending_deviation_score'] = (
    (df_fraud['amount_usd'] - df_fraud['past_avg_amount']) / (df_fraud['past_std_amount'] + 1e-6)
)

# 4.9. Velocity: ÑÑƒÐ¼Ð¼Ð° Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 24 Ñ‡Ð°ÑÐ°
def calc_velocity(group):
    times = group['timestamp'].values
    amounts = group['amount_usd'].values
    velocity = np.zeros(len(group))
    for i in range(1, len(group)):
        window_start = times[i] - timedelta(hours=24)
        recent_mask = (times[:i] >= window_start) & (times[:i] < times[i])
        velocity[i] = amounts[:i][recent_mask].sum() if recent_mask.any() else 0
    return pd.Series(velocity, index=group.index)

print("â³ Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ velocity_score (ÑÑƒÐ¼Ð¼Ð° Ð·Ð° 24 Ñ‡Ð°ÑÐ°)...")
df_fraud['velocity_score'] = df_fraud.groupby('customer_id', group_keys=False).progress_apply(calc_velocity)

# 4.10. isNewDevice: Ð½Ð¾Ð²Ð¾Ðµ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾ Ð´Ð»Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
def is_new_device(group):
    seen = set()
    result = []
    for dev in group['device']:
        result.append(1 if dev not in seen else 0)
        seen.add(dev)
    return result

df_fraud['isNewDevice'] = df_fraud.groupby('customer_id', group_keys=False)['device'].progress_apply(is_new_device).reset_index(level=0, drop=True)

print("âœ… Ð’ÑÐµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹.")

# --- 5. Ð’Ñ‹Ð±Ð¾Ñ€ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² ---
print("ðŸ“‹ Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð°...")

features = [
    # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ
    'amount_usd',
    'last_hour_num_transactions',
    'last_hour_total_amount',
    'last_hour_unique_merchants',
    'last_hour_unique_countries',
    'last_hour_max_single_amount',
    # ÐÐ¾Ð²Ñ‹Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ
    'time_since_last_transaction',
    'inactive_days',
    'account_age_days',
    # ÐŸÐ¾Ð²ÐµÐ´ÐµÐ½Ñ‡ÐµÑÐºÐ¸Ðµ
    'spending_deviation_score',
    'velocity_score',
    # Ð‘Ð¸Ð½Ð°Ñ€Ð½Ñ‹Ðµ Ñ€Ð¸ÑÐºÐ¸
    'is_card_present',
    'is_outside_home_country',
    'is_high_risk_vendor',
    'is_weekend',
    'has_device_fingerprint',
    'isNewDevice',
    'is_multi_country_hour',
    'is_high_merchant_burst',
    'is_high_risk_combo',
    # ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ
    'hour',
    'city_size',
    'card_type',
    'vendor_category'
]

# Ð¤Ð¸Ð»ÑŒÑ‚Ñ€: Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ
features = [f for f in features if f in df_fraud.columns]
X = df_fraud[features].copy()
y = df_fraud['is_fraud'].astype(int)

# --- 6. ÐšÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² ---
from sklearn.preprocessing import LabelEncoder

cat_cols = ['city_size', 'card_type', 'vendor_category']
le_dict = {}

for col in cat_cols:
    if col in X.columns:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
        le_dict[col] = le  # ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ñ‹

# Bool â†’ int
bool_cols = X.select_dtypes(include='bool').columns
X[bool_cols] = X[bool_cols].astype(int)

# Ð—Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¾Ð²
X = X.fillna(0)

print(f"âœ… X.shape: {X.shape}, y.shape: {y.shape}")

# --- 7. ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ---
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, f1_score, precision_recall_curve, auc
import matplotlib.pyplot as plt

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Ð’ÐµÑ ÐºÐ»Ð°ÑÑÐ°
scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])
print(f"âš–ï¸ Ð’ÐµÑ Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑÐ°: {scale_pos_weight:.2f}")

model = XGBClassifier(
    scale_pos_weight=scale_pos_weight,
    eval_metric='logloss',
    random_state=42,
    use_label_encoder=False,
    n_estimators=200,
    max_depth=7,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8
)

print("ðŸ‹ï¸ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸...")
model.fit(X_train, y_train)

# --- 8. ÐžÑ†ÐµÐ½ÐºÐ° ---
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred))

f1 = f1_score(y_test, y_pred)
print(f"F1-Score: {f1:.4f}")

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
pr_auc = auc(recall, precision)
print(f"PR-AUC: {pr_auc:.4f}")
# Ð“Ñ€Ð°Ñ„Ð¸Ðº PR-ÐºÑ€Ð¸Ð²Ð¾Ð¹
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# --- 9. Ð’Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² ---
print("\nðŸ” Ð¢Ð¾Ð¿-10 Ð²Ð°Ð¶Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²:")
importance_df = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False).head(10)
print(importance_df)

# --- 10. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ ---
print("ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²...")
joblib.dump(model, 'fraud_detection_model.pkl')
joblib.dump(le_dict, 'label_encoders.pkl')
joblib.dump(features, 'feature_names.pkl')

print("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¸ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹.")